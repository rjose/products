QPlan v5
========
Rino Jose <@rjose>
v1, Jul 25, 2013: Initial version

Overview
--------
I've been reading the
link:http://www.amazon.com/The-UNIX-Philosophy-ebook/dp/B002OL2G4G[Unix
Philsophy] again over the past few days. Something that's been nagging me for a
while with qplan is that I haven't written it in a unixy way. It has a captive
user interface, it doesn't interface with other programs, etc. In this iteration
of qplan, I'd like to try to make the pieces smaller and have the shell be the
primary interaction mechanism. We'll still need a server that can accept and
handle requests, but we'll interact with it differently from the commandline.
I'm thinking we should have a program that I can pipe text into that could be
interpreted as commands. We'd specify (or configure) the server/port/path that
these commands would be sent to. The program would then write the output from
the server to stdout. All the output should be in a form that can be easily
processed by awk (e.g., tab separated fields). 

If we organized our programs this way, we could have a separate formatting
program that would know how to take data (or sets of data) and prepare the
appropriate output. One would be the create more "standardized" output for
further processing. We should be able to switch this to an ascii report or to
JSON. I think these should be separate programs.

The server would also use the shell to pipe output from program to program to
generate a final result.

When we want to broadcast data, we could put the pipe together first, and then
have the final part of the pipe be something that could send data to others via
an HTTP request or a websocket. A websocket might be the way to go if we needed
to make decisions on what data to ask for based on some initial data from the
server -- in effect, if we needed the client and server to have a conversation
before getting the final data.

Implementation
--------------
. Write google doc spreadsheet to stdout [X][X][][]
. Condition data and pipe into mocked up qplan client [X][X][X]<X><X><X><X><X>
. Get minimal qplan server running based on v4 [][][][][][]
. Qplan client can update a qplan server [][][][][][][][]
. rbt is implemented as series of pipes [][][][][]

1 - Write google doc spreadsheet to stdout
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Let's see if we can just get something to work. I'll hardcode my password to
begin with -- I have to remember not to check this version in! The first step
is getting gspread to work. Let's see if we can get the sample working with
our google doc spreadsheet. OK, I got the spreadsheet data out (and tab
separated!). Let's move my username and password to a config file that only I
can read. I'll call this .gcat.conf. Let's see if I can add my info to this
and read it in. Done! Let's check this in.

Before we go on too much further, let's think about how we want to deal with
multiple spreadsheets. We could store two invocations of gcat in two variables
and then manipulate the data separately, but it would be really nice if we
could just store the immediate results directly at any point in the pipeline.
We'd have to include some header info that described the type of data from
each file. I'll look through the load.sh script to see how hard it would be to
incorprate this...I think we could have a single awk script that could store
data from various file types and then write them out in sections.

Our qplan_client could just make an HTTP request to the qplan_server with the
data. We'll need to add support for this to qplan (and a route). I wonder if
we could just use curl to make the request for us?


2 - Condition data and pipe into mocked up qplan_client
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Let's start by creating a qplan_source.ini file that lists spreadsheet key,
worksheet index, and type. We should be able to pipe this into gcat.py and
have it do the extractions. Done. Let's check in. Before we go on, let's
specify the sections to read in in the source.ini file itself.

Next up is conditioning the work data a la workify. We want to gather all work
data together. Let's start by handling different sections. I'm going to pipe
this into a file so I don't have to keep grabbing data. Now, let's see if we
can see the Work section.

After this, we'll handle staff data. The staff data is somewhat interesting. I
think I'll try to read the tracks out of the spreadsheet instead of hardcoding
them. Alright, I wrote the staff parsing code by hand. It's much more
resilient now.  Let's check this in.

The last piece here is writing the qplan client. Should we write this in
python, lua, or zsh? Let's do this in python. I want to split the data into
work data and staff data. After that, I'll strip off the headers and make two
requests to a server to update the data. I guess we can do a POST to
/work_items and one to /assignments.

Actually, I think we should do this in lua. That way, if I decide to go with
websockets for the communication with the server, I can use the code I'm
developing now. Also, I'm going to have an explicit updater app instead of a
generic client. Also, what I'll do is make the POST requests to a nonexistent
server and then in the next part bring over the code to run the server.


NOTE: Why did this take so long? Part of this was dealing with non-ASCII data from
the spreadsheet. Part of it was rewriting the staff parsing logic. Part of it
was just debugging.



Thoughts
--------
We should use link:https://github.com/burnash/gspread[gspread] to grab source
data from google docs.

I need to document the scripts so they can describe what's expected of them.

I also want to figure out how to test the pieces and the pipeline.
